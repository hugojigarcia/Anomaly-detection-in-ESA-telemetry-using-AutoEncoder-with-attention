{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def available_gpus():\n",
    "    gpus = torch.cuda.device_count()\n",
    "    return [torch.cuda.get_device_name(i) for i in range(gpus)]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"GPUs disponibles:\", available_gpus())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from libraries.utils import read_csv\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from metrics_libraries.metrics_calculator import MetricsCalculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISSION = 1\n",
    "PHASE = 4\n",
    "\n",
    "WINDOW_SIZE = 50\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "CHANNELS = [\"allchannels\", \"subset\", \"target\"][2]\n",
    "\n",
    "MODEL_SAVE_PATH = f\"../models/Mission{MISSION}-AutoEnconderFullWindow/Phase4_target_window50_percentile99_epochs25_lr0.0001__2025-01-14_10-43-52.pth\"\n",
    "# MODEL_SAVE_PATH = f\"../models/Mission{MISSION}-AutoEnconderLastEvent/Phase5_target_window50_percentile99_epochs25_lr0.0001__2025-01-14_00-02-38.pth\"\n",
    "# MODEL_SAVE_PATH = f\"../models/Mission{MISSION}-VariationalAutoencoderFullWindow/Phase5_target_window50_percentile99_epochs25_lr0.0001__2025-01-14_00-02-38.pth\"\n",
    "# MODEL_SAVE_PATH = f\"../models/Mission{MISSION}-VariationalAutoencoderLastEvent/Phase1_Channels18-28_window50_percentile99_epochs25_lr0.0001__2025-01-07_17-24-26.pth\"\n",
    "\n",
    "CHANNELS_INFO_PATH = f\"../data/Mission{MISSION}-ESA/channels.csv\"\n",
    "ESA_ANOMALIES_PATH = f\"../esa-anomalies/anomalies_mission{MISSION}.csv\"\n",
    "METRICS_SAVE_PATH = f\"../metrics/metrics.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_channel_number = 41 if MISSION == 1 else 18  # Only if CHANNELS == \"subset\" \n",
    "last_channel_number = 46 if MISSION == 1 else 28  # Only if CHANNELS == \"subset\"\n",
    "\n",
    "if CHANNELS == \"subset\":\n",
    "    input_data_path = f'../data/Mission{MISSION}-Preprocessed/data_preprocessed_channels{first_channel_number}_{last_channel_number}_frequency-previous_2000_{2013 if MISSION == 1 else 2003}.csv'\n",
    "else:\n",
    "    input_data_path = f'../data/Mission{MISSION}-Preprocessed/data_preprocessed_{CHANNELS}_frequency-previous_2000_{2013 if MISSION == 1 else 2003}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mission1_phases_dates = {\n",
    "    \"test_start_date\": \"2007-01-01T00:00:00\",\n",
    "    \"test_end_date\": \"2014-01-01T00:00:00\",\n",
    "\n",
    "    \"phase1_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase1_end_date_train\": \"2000-03-11T00:00:00\",\n",
    "    \"phase1_start_date_val\": \"2000-03-11T00:00:00\",\n",
    "    \"phase1_end_date_val\": \"2000-04-01T00:00:00\",\n",
    "    \n",
    "    \"phase2_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase2_end_date_train\": \"2000-09-01T00:00:00\",\n",
    "    \"phase2_start_date_val\": \"2000-09-01T00:00:00\",\n",
    "    \"phase2_end_date_val\": \"2000-11-01T00:00:00\",\n",
    "    \n",
    "    \"phase3_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase3_end_date_train\": \"2001-07-01T00:00:00\",\n",
    "    \"phase3_start_date_val\": \"2001-07-01T00:00:00\",\n",
    "    \"phase3_end_date_val\": \"2001-11-01T00:00:00\",\n",
    "    \n",
    "    \"phase4_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase4_end_date_train\": \"2003-04-01T00:00:00\",\n",
    "    \"phase4_start_date_val\": \"2003-04-01T00:00:00\",\n",
    "    \"phase4_end_date_val\": \"2003-07-01T00:00:00\",\n",
    "    \n",
    "    \"phase5_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase5_end_date_train\": \"2006-10-01T00:00:00\",\n",
    "    \"phase5_start_date_val\": \"2006-10-01T00:00:00\",\n",
    "    \"phase5_end_date_val\": \"2007-01-01T00:00:00\"\n",
    "}\n",
    "\n",
    "mission2_phases_dates = {\n",
    "    \"test_start_date\": \"2001-10-01T00:00:00\",\n",
    "    \"test_end_date\": \"2003-07-01T00:00:00\",\n",
    "\n",
    "    \"phase1_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase1_end_date_train\": \"2000-01-24T00:00:00\",\n",
    "    \"phase1_start_date_val\": \"2000-01-24T00:00:00\",\n",
    "    \"phase1_end_date_val\": \"2000-02-01T00:00:00\",\n",
    "    \n",
    "    \"phase2_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase2_end_date_train\": \"2000-05-01T00:00:00\",\n",
    "    \"phase2_start_date_val\": \"2000-05-01T00:00:00\",\n",
    "    \"phase2_end_date_val\": \"2000-06-01T00:00:00\",\n",
    "    \n",
    "    \"phase3_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase3_end_date_train\": \"2000-09-01T00:00:00\",\n",
    "    \"phase3_start_date_val\": \"2000-09-01T00:00:00\",\n",
    "    \"phase3_end_date_val\": \"2000-11-01T00:00:00\",\n",
    "    \n",
    "    \"phase4_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase4_end_date_train\": \"2001-07-01T00:00:00\",\n",
    "    \"phase4_start_date_val\": \"2001-07-01T00:00:00\",\n",
    "    \"phase4_end_date_val\": \"2001-10-01T00:00:00\"\n",
    "}\n",
    "\n",
    "missions_phases_dates = {\n",
    "    1: mission1_phases_dates,\n",
    "    2: mission2_phases_dates\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_train = pd.to_datetime(missions_phases_dates[MISSION][f\"phase{PHASE}_start_date_train\"])\n",
    "end_date_train = pd.to_datetime(missions_phases_dates[MISSION][f\"phase{PHASE}_end_date_train\"])\n",
    "start_date_val = pd.to_datetime(missions_phases_dates[MISSION][f\"phase{PHASE}_start_date_val\"])\n",
    "end_date_val = pd.to_datetime(missions_phases_dates[MISSION][f\"phase{PHASE}_end_date_val\"])\n",
    "start_date_test = pd.to_datetime(missions_phases_dates[MISSION][\"test_start_date\"])\n",
    "end_date_test = pd.to_datetime(missions_phases_dates[MISSION][\"test_end_date\"])\n",
    "\n",
    "if CHANNELS == \"target\":\n",
    "    channels_info = pd.read_csv(CHANNELS_INFO_PATH)\n",
    "    channels_list = list(channels_info[channels_info['Target']==\"YES\"]['Channel'])\n",
    "else:\n",
    "    channels_list = None if CHANNELS == \"allchannels\" else [f\"channel_{i}\" for i in range(first_channel_number, last_channel_number+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(MODEL_SAVE_PATH)\n",
    "model = checkpoint['model']   # Load the full model\n",
    "threshold_list = checkpoint['threshold']  # Access the threshold metadata\n",
    "scaler = checkpoint['scaler']  # Access the scaler metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv(input_data_path, sep=\";\")\n",
    "if channels_list is not None:\n",
    "    data = data[channels_list]\n",
    "\n",
    "# Filtrar los datos entre start_date_train y end_date_train\n",
    "data_train = data.loc[(data.index >= start_date_train) & (data.index < end_date_train)]\n",
    "data_val = data.loc[(data.index >= start_date_val) & (data.index < end_date_val)]\n",
    "data_test = data.loc[(data.index >= start_date_test) & (data.index < end_date_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_normalized = pd.DataFrame(scaler.transform(data_train), index=data_train.index, columns=data_train.columns)\n",
    "df_val_normalized = pd.DataFrame(scaler.transform(data_val), index=data_val.index, columns=data_val.columns)\n",
    "df_test_normalized = pd.DataFrame(scaler.transform(data_test), index=data_test.index, columns=data_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_train = model.predict(threshold_list, df_train_normalized, WINDOW_SIZE, BATCH_SIZE, device)\n",
    "anomalies_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_val = model.predict(threshold_list, df_val_normalized, WINDOW_SIZE, BATCH_SIZE, device)\n",
    "anomalies_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_test = model.predict(threshold_list, df_test_normalized, WINDOW_SIZE, BATCH_SIZE, device)\n",
    "anomalies_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_calculator = MetricsCalculator(ESA_ANOMALIES_PATH, CHANNELS_INFO_PATH, channels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_train = metrics_calculator.get_metrics(anomalies_train, start_date_train, end_date_train)\n",
    "metrics_calculator.print_metrics_table(metrics_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_val = metrics_calculator.get_metrics(anomalies_val, start_date_val, end_date_val)\n",
    "metrics_calculator.print_metrics_table(metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_test = metrics_calculator.get_metrics(anomalies_test, start_date_test, end_date_test)\n",
    "metrics_calculator.print_metrics_table(metrics_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "__model_type = MODEL_SAVE_PATH.split(\"/\")[-2].split(\"-\")[-1]\n",
    "__model_name = \".\".join(os.path.basename(MODEL_SAVE_PATH).split(\".\")[:-1])\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metrics, dataset in zip([metrics_train, metrics_val, metrics_test], [\"train\", \"val\", \"test\"]):\n",
    "    metrics[\"mission\"] = MISSION\n",
    "    metrics[\"phase\"] = PHASE\n",
    "    metrics[\"model_type\"] = __model_type\n",
    "    metrics[\"data\"] = dataset\n",
    "    metrics[\"model_name\"] = __model_name\n",
    "    metrics[\"timestamp\"] = timestamp\n",
    "    metrics[\"window_size\"] = WINDOW_SIZE\n",
    "    metrics[\"channels\"] = CHANNELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame([metrics_train])\n",
    "df_val = pd.DataFrame([metrics_val])\n",
    "df_test = pd.DataFrame([metrics_test])\n",
    "\n",
    "# Combinar los DataFrames\n",
    "df = pd.concat([df_train, df_val, df_test])\n",
    "new_columns = [\"mission\", \"phase\", \"model_type\", \"data\", \"model_name\", \"timestamp\", \"window_size\", \"channels\"]\n",
    "columns_order = new_columns + [col for col in df.columns if col not in new_columns]\n",
    "df = df[columns_order]\n",
    "\n",
    "# Guardar las mÃ©tricas en el archivo CSV\n",
    "if os.path.isfile(METRICS_SAVE_PATH):\n",
    "    # Si el archivo existe, agregar al final\n",
    "    df.to_csv(METRICS_SAVE_PATH, mode='a', header=False, index=False)\n",
    "else:\n",
    "    # Si no existe, crear el archivo con el encabezado\n",
    "    df.to_csv(METRICS_SAVE_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "__model_type = MODEL_SAVE_PATH.split(\"/\")[-2].split(\"-\")[-1]\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "test_type = f\"{CHANNELS}\" if CHANNELS == \"allchannels\" or CHANNELS == \"target\" else f\"Channels{first_channel_number}-{last_channel_number}\"\n",
    "\n",
    "__model_name = \".\".join(os.path.basename(MODEL_SAVE_PATH).split(\".\")[:-1])\n",
    "parameters = __model_name.split(\"_\")\n",
    "percentil = parameters[3][len(\"percentile\"):]\n",
    "epochs = parameters[4][len(\"epochs\"):]\n",
    "learning_rate = parameters[5][len(\"lr\"):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name = f\"Mission{MISSION}_{__model_type}_{__model_name}_{timestamp.replace(' ', '_').replace(':', '-')}\"\n",
    "run_name = f\"Mission{MISSION}_{__model_type}_{__model_name}\"\n",
    "run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"MET-ESA\",\n",
    "    id=run_name,\n",
    "    # id=\"oayk9f9m\",\n",
    "    name=run_name,\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"mission\": MISSION,\n",
    "        \"phase\": PHASE,\n",
    "        \"model_type\": __model_type,\n",
    "        \"model_name\": __model_name,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"channels\": test_type,\n",
    "        \"window_size\": WINDOW_SIZE,\n",
    "        \"percentile\": percentil,\n",
    "        \"epochs\": epochs,\n",
    "        \"learning_rate\": learning_rate,\n",
    "    },\n",
    "\n",
    "    resume=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = wandb.Table(columns= [\"model_name\", \"data\"] + list(metrics_train.keys()))\n",
    "table.add_data(__model_name, \"train\", *list(metrics_train.values()))\n",
    "table.add_data(__model_name, \"val\", *list(metrics_val.values()))\n",
    "table.add_data(__model_name, \"test\", *list(metrics_test.values()))\n",
    "wandb.log({\"metrics\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_train_renamed = {f\"train_{key}\": value for key, value in metrics_train.items()}\n",
    "metrics_val_renamed = {f\"val_{key}\": value for key, value in metrics_val.items()}\n",
    "metrics_test_renamed = {f\"test_{key}\": value for key, value in metrics_test.items()}\n",
    "\n",
    "wandb.log({**metrics_train_renamed, **metrics_val_renamed, **metrics_test_renamed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
