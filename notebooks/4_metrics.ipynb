{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def available_gpus():\n",
    "    gpus = torch.cuda.device_count()\n",
    "    return [torch.cuda.get_device_name(i) for i in range(gpus)]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"GPUs disponibles:\", available_gpus())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.utils import read_csv\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from metrics_libraries.metrics_calculator import MetricsCalculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISSION = 2\n",
    "PHASE = 1\n",
    "\n",
    "WINDOW_SIZE = 50\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "CHANNELS = [\"allchannels\", \"subset\", \"target\"][2]\n",
    "FIRST_CHANNEL_NUMBER = 18  # Only if CHANNELS == \"subset\" \n",
    "LAST_CHANNEL_NUMBER = 28  # Only if CHANNELS == \"subset\"\n",
    "\n",
    "MODEL_SAVE_PATH = f\"../models/AutoEnconderFullWindow/Phase1_Channels18-28_window50_epochs25_lr0.0001.pth\"\n",
    "CHANNELS_INFO_PATH = \"../data/Mission2-ESA/channels.csv\"\n",
    "ESA_ANOMALIES_PATH = \"../esa-anomalies/anomalies_mission2.csv\"\n",
    "INPUT_DATA_PATH = f'../data/Mission2-Preprocessed/data_preprocessed_target_frequency-previous_2000_2003.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mission1_phases_dates = {\n",
    "    \"test_start_date\": \"2007-01-01T00:00:00\",\n",
    "    \"test_end_date\": \"2014-01-01T00:00:00\",\n",
    "\n",
    "    \"phase1_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase1_end_date_train\": \"2000-03-11T00:00:00\",\n",
    "    \"phase1_start_date_val\": \"2000-03-11T00:00:00\",\n",
    "    \"phase1_end_date_val\": \"2000-04-01T00:00:00\",\n",
    "    \n",
    "    \"phase2_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase2_end_date_train\": \"2000-09-01T00:00:00\",\n",
    "    \"phase2_start_date_val\": \"2000-09-01T00:00:00\",\n",
    "    \"phase2_end_date_val\": \"2000-11-01T00:00:00\",\n",
    "    \n",
    "    \"phase3_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase3_end_date_train\": \"2001-07-01T00:00:00\",\n",
    "    \"phase3_start_date_val\": \"2001-07-01T00:00:00\",\n",
    "    \"phase3_end_date_val\": \"2001-11-01T00:00:00\",\n",
    "    \n",
    "    \"phase4_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase4_end_date_train\": \"2003-04-01T00:00:00\",\n",
    "    \"phase4_start_date_val\": \"2003-04-01T00:00:00\",\n",
    "    \"phase4_end_date_val\": \"2003-07-01T00:00:00\",\n",
    "    \n",
    "    \"phase5_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase5_end_date_train\": \"2006-10-01T00:00:00\",\n",
    "    \"phase5_start_date_val\": \"2006-10-01T00:00:00\",\n",
    "    \"phase5_end_date_val\": \"2007-01-01T00:00:00\"\n",
    "}\n",
    "\n",
    "mission2_phases_dates = {\n",
    "    \"test_start_date\": \"2001-10-01T00:00:00\",\n",
    "    \"test_end_date\": \"2003-07-01T00:00:00\",\n",
    "\n",
    "    \"phase1_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase1_end_date_train\": \"2000-01-24T00:00:00\",\n",
    "    \"phase1_start_date_val\": \"2000-01-24T00:00:00\",\n",
    "    \"phase1_end_date_val\": \"2000-02-01T00:00:00\",\n",
    "    \n",
    "    \"phase2_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase2_end_date_train\": \"2000-05-01T00:00:00\",\n",
    "    \"phase2_start_date_val\": \"2000-05-01T00:00:00\",\n",
    "    \"phase2_end_date_val\": \"2000-06-01T00:00:00\",\n",
    "    \n",
    "    \"phase3_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase3_end_date_train\": \"2000-09-01T00:00:00\",\n",
    "    \"phase3_start_date_val\": \"2000-09-01T00:00:00\",\n",
    "    \"phase3_end_date_val\": \"2000-11-01T00:00:00\",\n",
    "    \n",
    "    \"phase4_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase4_end_date_train\": \"2001-07-01T00:00:00\",\n",
    "    \"phase4_start_date_val\": \"2001-07-01T00:00:00\",\n",
    "    \"phase4_end_date_val\": \"2001-10-01T00:00:00\"\n",
    "}\n",
    "\n",
    "missions_phases_dates = {\n",
    "    1: mission1_phases_dates,\n",
    "    2: mission2_phases_dates\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_train = pd.to_datetime(missions_phases_dates[MISSION][f\"phase{PHASE}_start_date_train\"])\n",
    "end_date_train = pd.to_datetime(missions_phases_dates[MISSION][f\"phase{PHASE}_end_date_train\"])\n",
    "start_date_val = pd.to_datetime(missions_phases_dates[MISSION][f\"phase{PHASE}_start_date_val\"])\n",
    "end_date_val = pd.to_datetime(missions_phases_dates[MISSION][f\"phase{PHASE}_end_date_val\"])\n",
    "start_date_test = pd.to_datetime(missions_phases_dates[MISSION][\"test_start_date\"])\n",
    "end_date_test = pd.to_datetime(missions_phases_dates[MISSION][\"test_end_date\"])\n",
    "\n",
    "if CHANNELS == \"target\":\n",
    "    channels_info = pd.read_csv(CHANNELS_INFO_PATH)\n",
    "    channels_list = list(channels_info[channels_info['Target']==\"YES\"]['Channel'])\n",
    "else:\n",
    "    channels_list = None if CHANNELS == \"allchannels\" else [f\"channel_{i}\" for i in range(FIRST_CHANNEL_NUMBER, LAST_CHANNEL_NUMBER+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(MODEL_SAVE_PATH)\n",
    "model = checkpoint['model']   # Load the full model\n",
    "threshold_list = checkpoint['threshold']  # Access the threshold metadata\n",
    "scaler = checkpoint['scaler']  # Access the scaler metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv(INPUT_DATA_PATH, sep=\";\")\n",
    "if channels_list is not None:\n",
    "    data = data[channels_list]\n",
    "\n",
    "# Filtrar los datos entre start_date_train y end_date_train\n",
    "data_train = data.loc[(data.index >= start_date_train) & (data.index < end_date_train)]\n",
    "data_val = data.loc[(data.index >= start_date_val) & (data.index < end_date_val)]\n",
    "data_test = data.loc[(data.index >= start_date_test) & (data.index < end_date_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_normalized = pd.DataFrame(scaler.transform(data_train), index=data_train.index, columns=data_train.columns)\n",
    "df_val_normalized = pd.DataFrame(scaler.transform(data_val), index=data_val.index, columns=data_val.columns)\n",
    "df_test_normalized = pd.DataFrame(scaler.transform(data_test), index=data_test.index, columns=data_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_train = model.predict(threshold_list, df_train_normalized, WINDOW_SIZE, BATCH_SIZE, device)\n",
    "anomalies_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_val = model.predict(threshold_list, df_val_normalized, WINDOW_SIZE, BATCH_SIZE, device)\n",
    "anomalies_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_test = model.predict(threshold_list, df_test_normalized, WINDOW_SIZE, BATCH_SIZE, device)\n",
    "anomalies_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_calculator = MetricsCalculator(ESA_ANOMALIES_PATH, CHANNELS_INFO_PATH, channels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_train = metrics_calculator.get_metrics(anomalies_train, start_date_train, end_date_train)\n",
    "metrics_calculator.print_metrics_table(metrics_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_val = metrics_calculator.get_metrics(anomalies_val, start_date_val, end_date_val)\n",
    "metrics_calculator.print_metrics_table(anomalies_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_test = metrics_calculator.get_metrics(anomalies_test, start_date_test, end_date_test)\n",
    "metrics_calculator.print_metrics_table(anomalies_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
