{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def available_gpus():\n",
    "    gpus = torch.cuda.device_count()\n",
    "    return [torch.cuda.get_device_name(i) for i in range(gpus)]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"GPUs disponibles:\", available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from typing import List, Literal, Tuple\n",
    "\n",
    "import dash_bootstrap_components as dbc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "import polars as pl\n",
    "from dash import Dash, Input, Output, dcc, html\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly_resampler import FigureResampler\n",
    "from plotly_resampler.aggregation import MinMaxLTTB\n",
    "from libraries.utils import read_csv\n",
    "from jupyter_dash import JupyterDash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISSION = 2\n",
    "PHASE = 5\n",
    "\n",
    "WINDOW_SIZE = 50\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "CHANNELS = [\"allchannels\", \"subset\", \"target\"][2]\n",
    "\n",
    "MODEL_SAVE_PATH = f\"../models/Mission{MISSION}-AutoEnconderFullWindow/Phase4_target_window50_percentile99_epochs25_lr0.0001__2025-01-07_22-14-40.pth\"\n",
    "# MODEL_SAVE_PATH = f\"../models/Mission{MISSION}-AutoEnconderLastEvent/Phase3_Channels18-28_window50_percentile99_epochs25_lr0.0001__2025-01-07_14-06-25.pth\"\n",
    "# MODEL_SAVE_PATH = f\"../models/Mission{MISSION}-VariationalAutoencoderFullWindow/Phase1_Channels18-28_window50_percentile99_epochs25_lr0.0001__2025-01-07_16-24-43.pth\"\n",
    "# MODEL_SAVE_PATH = f\"../models/Mission{MISSION}-VariationalAutoencoderLastEvent/Phase1_Channels18-28_window50_percentile99_epochs25_lr0.0001__2025-01-07_17-24-26.pth\"\n",
    "\n",
    "CHANNELS_INFO_PATH = f\"../data/Mission{MISSION}-ESA/channels.csv\"\n",
    "ESA_ANOMALIES_PATH = f\"../esa-anomalies/anomalies_mission{MISSION}.csv\"\n",
    "METRICS_SAVE_PATH = f\"../metrics/metrics.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_channel_number = 41 if MISSION == 1 else 18  # Only if CHANNELS == \"subset\" \n",
    "last_channel_number = 46 if MISSION == 1 else 28  # Only if CHANNELS == \"subset\"\n",
    "\n",
    "if CHANNELS == \"subset\":\n",
    "    input_data_path = f'../data/Mission{MISSION}-Preprocessed/data_preprocessed_channels{first_channel_number}_{last_channel_number}_frequency-previous_2000_{2013 if MISSION == 1 else 2003}.csv'\n",
    "else:\n",
    "    input_data_path = f'../data/Mission{MISSION}-Preprocessed/data_preprocessed_{CHANNELS}_frequency-previous_2000_{2013 if MISSION == 1 else 2003}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mission1_phases_dates = {\n",
    "    \"test_start_date\": \"2007-01-01T00:00:00\",\n",
    "    \"test_end_date\": \"2014-01-01T00:00:00\",\n",
    "\n",
    "    \"phase1_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase1_end_date_train\": \"2000-03-11T00:00:00\",\n",
    "    \"phase1_start_date_val\": \"2000-03-11T00:00:00\",\n",
    "    \"phase1_end_date_val\": \"2000-04-01T00:00:00\",\n",
    "    \n",
    "    \"phase2_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase2_end_date_train\": \"2000-09-01T00:00:00\",\n",
    "    \"phase2_start_date_val\": \"2000-09-01T00:00:00\",\n",
    "    \"phase2_end_date_val\": \"2000-11-01T00:00:00\",\n",
    "    \n",
    "    \"phase3_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase3_end_date_train\": \"2001-07-01T00:00:00\",\n",
    "    \"phase3_start_date_val\": \"2001-07-01T00:00:00\",\n",
    "    \"phase3_end_date_val\": \"2001-11-01T00:00:00\",\n",
    "    \n",
    "    \"phase4_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase4_end_date_train\": \"2003-04-01T00:00:00\",\n",
    "    \"phase4_start_date_val\": \"2003-04-01T00:00:00\",\n",
    "    \"phase4_end_date_val\": \"2003-07-01T00:00:00\",\n",
    "    \n",
    "    \"phase5_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase5_end_date_train\": \"2006-10-01T00:00:00\",\n",
    "    \"phase5_start_date_val\": \"2006-10-01T00:00:00\",\n",
    "    \"phase5_end_date_val\": \"2007-01-01T00:00:00\"\n",
    "}\n",
    "\n",
    "mission2_phases_dates = {\n",
    "    \"test_start_date\": \"2001-10-01T00:00:00\",\n",
    "    \"test_end_date\": \"2003-07-01T00:00:00\",\n",
    "\n",
    "    \"phase1_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase1_end_date_train\": \"2000-01-24T00:00:00\",\n",
    "    \"phase1_start_date_val\": \"2000-01-24T00:00:00\",\n",
    "    \"phase1_end_date_val\": \"2000-02-01T00:00:00\",\n",
    "    \n",
    "    \"phase2_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase2_end_date_train\": \"2000-05-01T00:00:00\",\n",
    "    \"phase2_start_date_val\": \"2000-05-01T00:00:00\",\n",
    "    \"phase2_end_date_val\": \"2000-06-01T00:00:00\",\n",
    "    \n",
    "    \"phase3_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase3_end_date_train\": \"2000-09-01T00:00:00\",\n",
    "    \"phase3_start_date_val\": \"2000-09-01T00:00:00\",\n",
    "    \"phase3_end_date_val\": \"2000-11-01T00:00:00\",\n",
    "    \n",
    "    \"phase4_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase4_end_date_train\": \"2001-07-01T00:00:00\",\n",
    "    \"phase4_start_date_val\": \"2001-07-01T00:00:00\",\n",
    "    \"phase4_end_date_val\": \"2001-10-01T00:00:00\"\n",
    "}\n",
    "\n",
    "missions_phases_dates = {\n",
    "    1: mission1_phases_dates,\n",
    "    2: mission2_phases_dates\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime(missions_phases_dates[MISSION][f\"phase1_start_date_train\"])\n",
    "end_date = pd.to_datetime(missions_phases_dates[MISSION][\"test_end_date\"])\n",
    "end_date = pd.to_datetime(\"2000-03-01T00:00:00\") # TODO quitar\n",
    "\n",
    "if CHANNELS == \"target\":\n",
    "    channels_info = pd.read_csv(CHANNELS_INFO_PATH)\n",
    "    channels_list = list(channels_info[channels_info['Target']==\"YES\"]['Channel'])\n",
    "else:\n",
    "    channels_list = None if CHANNELS == \"allchannels\" else [f\"channel_{i}\" for i in range(first_channel_number, last_channel_number+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(MODEL_SAVE_PATH)\n",
    "model = checkpoint['model']   # Load the full model\n",
    "threshold_list = checkpoint['threshold']  # Access the threshold metadata\n",
    "scaler = checkpoint['scaler']  # Access the scaler metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv(input_data_path, sep=\";\")\n",
    "if channels_list is not None:\n",
    "    data = data[channels_list]\n",
    "\n",
    "# Filtrar los datos entre start_date_train y end_date_train\n",
    "data = data.loc[(data.index >= start_date) & (data.index < end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = pd.DataFrame(scaler.transform(data), index=data.index, columns=data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = model.predict(threshold_list, df_normalized, WINDOW_SIZE, BATCH_SIZE, device)\n",
    "anomalies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def format_anomalies(anomalies: pd.DataFrame) -> pd.DataFrame:\n",
    "    formatted_data = []\n",
    "\n",
    "    # Iterar sobre cada canal (columna)\n",
    "    for channel in anomalies.columns:\n",
    "        channel_data = anomalies[channel]\n",
    "        is_active = False  # Para rastrear si estamos dentro de una secuencia activa\n",
    "        start_time = None  # Almacenar el tiempo de inicio de la anomalía\n",
    "\n",
    "        # Iterar por cada fila en el canal\n",
    "        for time, value in channel_data.items():\n",
    "            if value == 1 and not is_active:\n",
    "                # Detectamos el inicio de una anomalía\n",
    "                is_active = True\n",
    "                start_time = time\n",
    "            elif value == 0 and is_active:\n",
    "                # Detectamos el final de una anomalía\n",
    "                is_active = False\n",
    "                end_time = time\n",
    "                # Guardar el resultado\n",
    "                formatted_data.append({\"Channel\": channel, \"StartTime\": start_time, \"EndTime\": end_time})\n",
    "\n",
    "        # Manejar el caso en que una anomalía sigue activa hasta el final del DataFrame\n",
    "        if is_active:\n",
    "            formatted_data.append({\"Channel\": channel, \"StartTime\": start_time, \"EndTime\": channel_data.index[-1]})\n",
    "\n",
    "    # Convertir los resultados en un nuevo DataFrame\n",
    "    anomalies_formatted = pd.DataFrame(formatted_data)\n",
    "\n",
    "    # Ordenar el DataFrame por StartTime\n",
    "    anomalies_formatted = anomalies_formatted.sort_values(by=\"StartTime\").reset_index(drop=True)\n",
    "\n",
    "    return anomalies_formatted\n",
    "\n",
    "df_anomalies_predicted = format_anomalies(anomalies)\n",
    "df_anomalies_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ESA anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Auxiliar functions\n",
    "# ---------------------------------------------------\n",
    "def import_data(\n",
    "    input_data_path: str,\n",
    "    channels: List[str] = None,\n",
    "    start_date: date = None,\n",
    "    end_date: date = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Import data from a csv file  and filter it by channels and date.\n",
    "\n",
    "    Args:\n",
    "        input_data_path (str): path to the data file\n",
    "        channels (List[str], optional): list of channels to filter. Defaults to None.\n",
    "        start_date (date, optional): start date to filter the data. Defaults to None.\n",
    "        end_date (date, optional): end date to filter the data. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe with the data\n",
    "    \"\"\"\n",
    "    result = pl.scan_csv(input_data_path, separator=\";\", n_rows=10)\n",
    "\n",
    "    schema = {}\n",
    "    schema[\"time\"] = pl.Datetime\n",
    "    columns = result.collect_schema().names()\n",
    "\n",
    "    for column in columns:\n",
    "        if \"time\" not in column:\n",
    "            schema[column] = pl.Float32\n",
    "\n",
    "    result = pl.scan_csv(input_data_path, separator=\";\", schema=schema)\n",
    "\n",
    "    if start_date is not None:\n",
    "        result = result.filter(pl.col(\"time\") >= start_date)\n",
    "        result = result.filter(pl.col(\"time\") <= end_date)\n",
    "\n",
    "    if channels is not None:\n",
    "        result = result.select([\"time\"] + channels)\n",
    "\n",
    "    result = result.collect()\n",
    "    result = result.to_pandas()\n",
    "    result.set_index(\"time\", inplace=True)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Code: Read data\n",
    "# ---------------------------------------------------\n",
    "\n",
    "## Read preprcessed data\n",
    "df_data= import_data(\n",
    "    input_data_path=input_data_path,\n",
    "    channels=channels_list,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    ")\n",
    "\n",
    "\n",
    "## Import df anomalies\n",
    "df_anomalies_esa = pd.read_csv(ESA_ANOMALIES_PATH)\n",
    "df_anomalies_esa = df_anomalies_esa[\n",
    "    (df_anomalies_esa[\"Category\"] == \"Anomaly\") | (df_anomalies_esa[\"Category\"] == \"Rare Event\")\n",
    "]\n",
    "if channels_list is not None:\n",
    "    df_anomalies_esa = df_anomalies_esa[df_anomalies_esa[\"Channel\"].isin(channels_list)]\n",
    "\n",
    "\n",
    "\n",
    "## Import predictions\n",
    "if channels_list is not None:\n",
    "    df_anomalies_predicted = df_anomalies_predicted[df_anomalies_predicted[\"Channel\"].isin(channels_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Code: Dashboard\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# If no ESA anomalies are found in the selected date range, an error message is displayed.\n",
    "\n",
    "# anomaly_category_colors\n",
    "anomaly_category_colors = {\n",
    "    \"Rare Event\": \"blue\",\n",
    "    \"Anomaly\": \"red\",\n",
    "    \"Communication Gap\": \"green\",\n",
    "}\n",
    "# app = Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "\n",
    "app.layout = html.Div(\n",
    "    [\n",
    "        dbc.Alert(\n",
    "            \"No anomalies found in the filtered data\",\n",
    "            color=\"danger\",\n",
    "            id=\"error-banner\",\n",
    "            is_open=False,\n",
    "        ),\n",
    "        dcc.DatePickerRange(\n",
    "            id=\"date-picker-range\",\n",
    "            start_date=date(2000, 2, 1),\n",
    "            end_date=date(2000, 2, 29),\n",
    "            display_format=\"DD-MM-YYYY\",\n",
    "        ),\n",
    "        dcc.Graph(id=\"time-series-graph\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    [Output(\"error-banner\", \"is_open\"), Output(\"time-series-graph\", \"figure\")],\n",
    "    [Input(\"date-picker-range\", \"start_date\"), Input(\"date-picker-range\", \"end_date\")],\n",
    ")\n",
    "def update_graph(start_date: str, end_date: str) -> Tuple[bool, go.Figure]:\n",
    "    # data filtered by date\n",
    "    filtered_df_data = df_data[(df_data.index >= start_date) & (df_data.index <= end_date)]\n",
    "\n",
    "    # esa anomalies filtered by date\n",
    "    filtered_anomalies_esa = df_anomalies_esa[\n",
    "        (df_anomalies_esa[\"StartTime\"] >= start_date)\n",
    "        & (df_anomalies_esa[\"EndTime\"] <= end_date)\n",
    "    ]\n",
    "\n",
    "    # predicted anomalies filtered by date\n",
    "    filtered_anomalies_predicted = df_anomalies_predicted[\n",
    "        (df_anomalies_predicted[\"StartTime\"] >= start_date)\n",
    "        & (df_anomalies_predicted[\"EndTime\"] <= end_date)\n",
    "    ]\n",
    "\n",
    "    # channels = filtered_anomalies_esa.Channel.unique()\n",
    "    channels1 = list(filtered_anomalies_esa.Channel)\n",
    "    channels2 = list(filtered_anomalies_predicted.Channel)\n",
    "    channels = np.array(list(set(channels1 + channels2)))\n",
    "    channels = np.array(sorted(channels, key=lambda x: int(x.split('_')[1])))\n",
    "    # channels.sort()\n",
    "    if len(channels) == 0:\n",
    "        return True, go.Figure()\n",
    "    \n",
    "    # esa_anomalies = filtered_anomalies_esa.ID.unique()\n",
    "    # esa_anomalies.sort()\n",
    "\n",
    "    # Supblots. The number of rows is the number of channels + 1 (SPE plot from PitIA)\n",
    "    fig = FigureResampler(\n",
    "        make_subplots(\n",
    "            # rows=len(channels) + 1 + len(all_channels),\n",
    "            rows=len(channels),\n",
    "            cols=1,\n",
    "            shared_xaxes=\"columns\",\n",
    "            subplot_titles=channels,\n",
    "            vertical_spacing=0.01,\n",
    "        ),\n",
    "        default_downsampler=MinMaxLTTB(parallel=True),\n",
    "        create_overview=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    for i, channel in enumerate(channels):\n",
    "        fig.add_trace(\n",
    "            go.Scattergl(\n",
    "                name=channel,\n",
    "                showlegend=True,\n",
    "                x=filtered_df_data.index,\n",
    "                y=filtered_df_data[channel],\n",
    "            ),\n",
    "            hf_x=filtered_df_data.index,\n",
    "            hf_y=filtered_df_data[channel],\n",
    "            # row=i + 2,\n",
    "            row=i + 1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        esa_anomalies = filtered_anomalies_esa[filtered_anomalies_esa[\"Channel\"] == channel]\n",
    "        for _, anomaly_ in esa_anomalies.iterrows():\n",
    "            fig.add_shape(\n",
    "                type=\"rect\",\n",
    "                # x0=max([start_date, anomaly_[\"StartTime\"].values[0]]),\n",
    "                x0=max([start_date, anomaly_[\"StartTime\"]]),\n",
    "                y0=filtered_df_data[channel].min(),\n",
    "                # x1=min([end_date, anomaly_[\"EndTime\"].values[0]]),\n",
    "                x1=min([end_date, anomaly_[\"EndTime\"]]),\n",
    "                y1=filtered_df_data[channel].max(),\n",
    "                # line=dict(color=\"red\", width=1),\n",
    "                line=dict(color=anomaly_category_colors[anomaly_[\"Category\"]], width=1),\n",
    "                # fillcolor=anomaly_category_colors[anomaly_[\"Category\"].values[0]],\n",
    "                fillcolor=\"yellow\",\n",
    "                opacity=0.25,\n",
    "                # row=i + 2,\n",
    "                row=i + 1,\n",
    "                col=1,\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Scattergl(\n",
    "                    # x=[anomaly_[\"StartTime\"].values[0]],\n",
    "                    x=[anomaly_[\"StartTime\"]],\n",
    "                    y=[\n",
    "                        filtered_df_data[channel].min() + filtered_df_data[channel].min() / 10\n",
    "                    ],\n",
    "                    mode=\"text\",\n",
    "                    marker=dict(size=8),\n",
    "                    # text=anomaly_[\"Category\"].values[0]\n",
    "                    text=anomaly_[\"Category\"]\n",
    "                    + \" \"\n",
    "                    # + anomaly_[\"ID\"].values[0],\n",
    "                    + anomaly_[\"ID\"],\n",
    "                    line=dict(color=\"black\", width=1),\n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                # row=i + 2,\n",
    "                row=i + 1,\n",
    "                col=1,\n",
    "            )\n",
    "    \n",
    "        predicted_anomalies = filtered_anomalies_predicted[filtered_anomalies_predicted[\"Channel\"] == channel]\n",
    "        for _, anomaly_ in predicted_anomalies.iterrows():\n",
    "            fig.add_shape(\n",
    "                type=\"rect\",\n",
    "                # x0=max([start_date, anomaly_[\"StartTime\"].values[0]]),\n",
    "                x0=max([start_date, str(anomaly_[\"StartTime\"])]),\n",
    "                y0=filtered_df_data[channel].min(),\n",
    "                # x1=min([end_date, anomaly_[\"EndTime\"].values[0]]),\n",
    "                x1=min([end_date, str(anomaly_[\"EndTime\"])]),\n",
    "                y1=filtered_df_data[channel].max(),\n",
    "                line=dict(color=\"red\", width=1),\n",
    "                # fillcolor=anomaly_category_colors[anomaly_[\"Category\"].values[0]],\n",
    "                fillcolor=\"blue\",\n",
    "                opacity=0.25,\n",
    "                # row=i + 2,\n",
    "                row=i + 1,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "    for annotation in fig[\"layout\"][\"annotations\"]:\n",
    "        annotation[\"xanchor\"] = \"left\"\n",
    "        annotation[\"x\"] = 0\n",
    "    HEIGHT = 250 * len(channels) if len(channels) < 10 else 2000\n",
    "    fig.update_layout(height=HEIGHT)\n",
    "    return False, fig\n",
    "\n",
    "\n",
    "app.run_server(debug=True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
