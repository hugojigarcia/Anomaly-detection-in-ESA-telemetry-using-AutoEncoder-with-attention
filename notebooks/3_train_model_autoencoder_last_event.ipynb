{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def available_gpus():\n",
    "    gpus = torch.cuda.device_count()\n",
    "    return [torch.cuda.get_device_name(i) for i in range(gpus)]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"GPUs disponibles:\", available_gpus())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from libraries.utils import read_csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from libraries.sequence_generators import sequence_generator_last_event\n",
    "from models_architectures.AutoEnconderLastEvent import AutoEnconderLastEvent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISSION = 2\n",
    "PHASE = 4\n",
    "\n",
    "WINDOW_SIZE = 100\n",
    "PERCENTILE = 99\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 25\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "CHANNELS = [\"allchannels\", \"subset\", \"target\"][1]\n",
    "FIRST_CHANNEL_NUMBER = 18  # Only if CHANNELS == \"subset\" \n",
    "LAST_CHANNEL_NUMBER = 28  # Only if CHANNELS == \"subset\"\n",
    "\n",
    "# INPUT_DATA_PATH = f'../data/Mission2-Preprocessed/data_preprocessed_target_frequency-previous_2000_2003.csv'\n",
    "INPUT_DATA_PATH = f'../data/Mission2-Preprocessed/data_preprocessed_channels18_28_frequency-previous_2000_2003.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mission1_phases_dates = {\n",
    "    \"test_start_date\": \"2007-01-01T00:00:00\",\n",
    "    \"test_end_date\": \"2014-01-01T00:00:00\",\n",
    "\n",
    "    \"phase1_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase1_end_date_train\": \"2000-03-11T00:00:00\",\n",
    "    \"phase1_start_date_val\": \"2000-03-11T00:00:00\",\n",
    "    \"phase1_end_date_val\": \"2000-04-01T00:00:00\",\n",
    "    \n",
    "    \"phase2_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase2_end_date_train\": \"2000-09-01T00:00:00\",\n",
    "    \"phase2_start_date_val\": \"2000-09-01T00:00:00\",\n",
    "    \"phase2_end_date_val\": \"2000-11-01T00:00:00\",\n",
    "    \n",
    "    \"phase3_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase3_end_date_train\": \"2001-07-01T00:00:00\",\n",
    "    \"phase3_start_date_val\": \"2001-07-01T00:00:00\",\n",
    "    \"phase3_end_date_val\": \"2001-11-01T00:00:00\",\n",
    "    \n",
    "    \"phase4_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase4_end_date_train\": \"2003-04-01T00:00:00\",\n",
    "    \"phase4_start_date_val\": \"2003-04-01T00:00:00\",\n",
    "    \"phase4_end_date_val\": \"2003-07-01T00:00:00\",\n",
    "    \n",
    "    \"phase5_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase5_end_date_train\": \"2006-10-01T00:00:00\",\n",
    "    \"phase5_start_date_val\": \"2006-10-01T00:00:00\",\n",
    "    \"phase5_end_date_val\": \"2007-01-01T00:00:00\"\n",
    "}\n",
    "\n",
    "mission2_phases_dates = {\n",
    "    \"test_start_date\": \"2001-10-01T00:00:00\",\n",
    "    \"test_end_date\": \"2003-07-01T00:00:00\",\n",
    "\n",
    "    \"phase1_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase1_end_date_train\": \"2000-01-24T00:00:00\",\n",
    "    \"phase1_start_date_val\": \"2000-01-24T00:00:00\",\n",
    "    \"phase1_end_date_val\": \"2000-02-01T00:00:00\",\n",
    "    \n",
    "    \"phase2_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase2_end_date_train\": \"2000-05-01T00:00:00\",\n",
    "    \"phase2_start_date_val\": \"2000-05-01T00:00:00\",\n",
    "    \"phase2_end_date_val\": \"2000-06-01T00:00:00\",\n",
    "    \n",
    "    \"phase3_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase3_end_date_train\": \"2000-09-01T00:00:00\",\n",
    "    \"phase3_start_date_val\": \"2000-09-01T00:00:00\",\n",
    "    \"phase3_end_date_val\": \"2000-11-01T00:00:00\",\n",
    "    \n",
    "    \"phase4_start_date_train\": \"2000-01-01T00:00:00\",\n",
    "    \"phase4_end_date_train\": \"2001-07-01T00:00:00\",\n",
    "    \"phase4_start_date_val\": \"2001-07-01T00:00:00\",\n",
    "    \"phase4_end_date_val\": \"2001-10-01T00:00:00\"\n",
    "}\n",
    "\n",
    "missions_phases_dates = {\n",
    "    1: mission1_phases_dates,\n",
    "    2: mission2_phases_dates\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_train = datetime.fromisoformat(missions_phases_dates[MISSION][f\"phase{PHASE}_start_date_train\"])\n",
    "end_date_train = datetime.fromisoformat(missions_phases_dates[MISSION][f\"phase{PHASE}_end_date_train\"])\n",
    "start_date_val = datetime.fromisoformat(missions_phases_dates[MISSION][f\"phase{PHASE}_start_date_val\"])\n",
    "end_date_val = datetime.fromisoformat(missions_phases_dates[MISSION][f\"phase{PHASE}_end_date_val\"])\n",
    "\n",
    "test_type = f\"{CHANNELS}\" if CHANNELS == \"allchannels\" or CHANNELS == \"target\" else f\"Channels{FIRST_CHANNEL_NUMBER}-{LAST_CHANNEL_NUMBER}\"\n",
    "model_save_path = f\"../models/AutoEnconderLastEvent/Phase{PHASE}_{test_type}_window{WINDOW_SIZE}_percentile_{PERCENTILE}_epochs{EPOCHS}_lr{LEARNING_RATE}.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv(INPUT_DATA_PATH, sep=\";\")\n",
    "\n",
    "# Filtrar los datos entre start_date_train y end_date_train\n",
    "train_data = data.loc[(data.index >= start_date_train) & (data.index < end_date_train)]\n",
    "val_data = data.loc[(data.index >= start_date_val) & (data.index < end_date_val)]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Normalización o estandarización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar los datos entre 0 y 1\n",
    "scaler = MinMaxScaler()\n",
    "train_data_normalized = scaler.fit_transform(train_data)\n",
    "train_df_normalized = pd.DataFrame(train_data_normalized, index=train_data.index, columns=train_data.columns)\n",
    "val_data_normalized = scaler.transform(val_data)\n",
    "val_df_normalized = pd.DataFrame(val_data_normalized, index=val_data.index, columns=val_data.columns)\n",
    "train_df_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Transformer Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensiones del modelo\n",
    "sequence_length = WINDOW_SIZE\n",
    "n_features = train_df_normalized.shape[1]\n",
    "latent_dim = 8\n",
    "\n",
    "# Crear modelo\n",
    "autoencoder = AutoEnconderLastEvent(sequence_length, n_features, latent_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch_train = (len(train_df_normalized) - WINDOW_SIZE) // BATCH_SIZE\n",
    "\n",
    "def train_autoencoder(train_generator, val_generator, model, criterion, optimizer, epochs, steps_per_epoch_train, steps_per_epoch_val, device):\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_train_loss = 0  # Initialize total training loss for the epoch\n",
    "\n",
    "        # Training loop\n",
    "        for step in range(steps_per_epoch_train):\n",
    "            inputs, targets = next(train_generator)\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()  # Accumulate training loss\n",
    "\n",
    "        avg_train_loss = total_train_loss / steps_per_epoch_train  # Calculate average training loss\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "\n",
    "        # Validation loop (if val_generator is provided)\n",
    "        if val_generator is not None:\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "            total_val_loss = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for step in range(steps_per_epoch_val):\n",
    "                    val_inputs, val_targets = next(val_generator)\n",
    "                    val_inputs, val_targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss = criterion(val_outputs, val_targets)\n",
    "\n",
    "                    total_val_loss += val_loss.item()\n",
    "\n",
    "            avg_val_loss = total_val_loss / steps_per_epoch_val  # Calculate average validation loss\n",
    "            val_loss_history.append(avg_val_loss)\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    return train_loss_history, val_loss_history\n",
    "\n",
    "\n",
    "# Crea el generador de secuencias\n",
    "train_gen = sequence_generator_last_event(train_df_normalized.values, WINDOW_SIZE, BATCH_SIZE)\n",
    "val_gen = sequence_generator_last_event(val_df_normalized.values, WINDOW_SIZE, BATCH_SIZE)\n",
    "steps_per_epoch_val = (len(val_df_normalized) - WINDOW_SIZE) // BATCH_SIZE\n",
    "# Entrenar el modelo\n",
    "train_loss_history, val_loss_history = train_autoencoder(train_gen, val_gen, autoencoder, criterion, optimizer, epochs=EPOCHS,\n",
    "                  steps_per_epoch_train=steps_per_epoch_train, steps_per_epoch_val=steps_per_epoch_val, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_history(train_loss_history, val_loss_history=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss history.\n",
    "\n",
    "    Args:\n",
    "        train_loss_history (list): List of training loss values per epoch.\n",
    "        val_loss_history (list, optional): List of validation loss values per epoch. Default is None.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_loss_history) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_loss_history, label='Training Loss', marker='o', color='blue')\n",
    "    \n",
    "    if val_loss_history is not None and len(val_loss_history) > 0:\n",
    "        plt.plot(epochs, val_loss_history, label='Validation Loss', marker='o', color='orange')\n",
    "\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_save_path = \"../graphics/AutoEnconderLastEvent_\" + os.path.splitext(os.path.basename(model_save_path))[0] + \".png\"\n",
    "plot_loss_history(train_loss_history, val_loss_history, save_path=plot_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Calcular threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### THRESHOLD GENERAL\n",
    "# def calculate_general_threshold(percentile, autoencoder, df_normalized, window_size, batch_size, device):\n",
    "#     steps_per_epoch = (len(df_normalized) - window_size) // batch_size\n",
    "\n",
    "#     # Umbral basado en el percentil 95 del error\n",
    "#     train_gen = sequence_generator_last_event(df_normalized.values, window_size, batch_size)\n",
    "#     reconstruction_errors = []\n",
    "\n",
    "#     # Barra de progreso para el cálculo de reconstruction_errors\n",
    "#     for _ in tqdm(range(steps_per_epoch), desc=\"Calculando errores de reconstrucción\"):\n",
    "#         batch_inputs, batch_targets = next(train_gen)\n",
    "#         reconstructed_batch = autoencoder(batch_inputs.to(device)).detach().cpu().numpy()\n",
    "#         batch_targets = batch_targets.cpu().numpy()\n",
    "#         reconstruction_errors.extend(\n",
    "#             np.mean(np.square(batch_targets - reconstructed_batch), axis=(0))\n",
    "#         )\n",
    "\n",
    "#     return np.percentile(reconstruction_errors, percentile)\n",
    "\n",
    "# threshold = calculate_general_threshold(PERCENTILE, autoencoder, train_df_normalized, WINDOW_SIZE, BATCH_SIZE, device)\n",
    "# threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### THRESHOLD PERCENTILE\n",
    "# def calculate_channels_thresholds(percentile, autoencoder, df_normalized, window_size, batch_size, device):\n",
    "#     steps_per_epoch = (len(df_normalized) - window_size) // batch_size\n",
    "\n",
    "#     # Umbral basado en el percentil 95 del error\n",
    "#     train_gen = sequence_generator_last_event(df_normalized.values, window_size, batch_size)\n",
    "#     reconstruction_errors = []\n",
    "\n",
    "#     # Barra de progreso para el cálculo de reconstruction_errors\n",
    "#     for _ in tqdm(range(steps_per_epoch), desc=\"Calculando errores de reconstrucción\"):\n",
    "#         batch_inputs, batch_targets = next(train_gen)  # Obtener entradas y objetivos\n",
    "#         batch_inputs = batch_inputs.to(device)\n",
    "\n",
    "#         # Reconstruir el batch\n",
    "#         reconstructed_batch = autoencoder(batch_inputs).detach().cpu().numpy()  # Salidas reconstruidas\n",
    "#         batch_targets = batch_targets.cpu().numpy()\n",
    "\n",
    "#         errors = np.square(batch_targets - reconstructed_batch)  # Error por canal\n",
    "#         reconstruction_errors.extend(errors)\n",
    "\n",
    "#     return list(np.percentile(reconstruction_errors, percentile, axis=0))\n",
    "\n",
    "# threshold_list = calculate_channels_thresholds(PERCENTILE, autoencoder, train_df_normalized, WINDOW_SIZE, BATCH_SIZE, device)\n",
    "# threshold_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THRESHOLD IPR\n",
    "def anomaly_limits(errors, percentile, axis) -> float:\n",
    "    P1 = np.percentile(errors, 100-percentile, axis)\n",
    "    P2 = np.percentile(errors, percentile, axis)\n",
    "    IPR = P2 - P1\n",
    "    return list(P2 + 1.5 * IPR)\n",
    "\n",
    "def calculate_channels_thresholds(percentile, autoencoder, df_normalized, window_size, batch_size, device):\n",
    "    steps_per_epoch = (len(df_normalized) - window_size) // batch_size\n",
    "\n",
    "    # Umbral basado en el percentil 95 del error\n",
    "    train_gen = sequence_generator_last_event(df_normalized.values, window_size, batch_size)\n",
    "    reconstruction_errors = []\n",
    "\n",
    "    # Barra de progreso para el cálculo de reconstruction_errors\n",
    "    for _ in tqdm(range(steps_per_epoch), desc=\"Calculando errores de reconstrucción\"):\n",
    "        batch_inputs, batch_targets = next(train_gen)\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "\n",
    "        reconstructed_batch = autoencoder(batch_inputs).detach().cpu().numpy()\n",
    "        batch_targets = batch_targets.cpu().numpy()\n",
    "\n",
    "        reconstruction_errors.extend(\n",
    "            # np.mean(np.square(batch - reconstructed_batch), axis=(1, 2))\n",
    "            np.square(batch_targets - reconstructed_batch)\n",
    "        )\n",
    "    return anomaly_limits(reconstruction_errors, percentile, axis=(0))\n",
    "\n",
    "threshold_list = calculate_channels_thresholds(PERCENTILE, autoencoder, train_df_normalized, WINDOW_SIZE, BATCH_SIZE, device)\n",
    "threshold_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model': autoencoder,\n",
    "    'threshold': threshold_list,\n",
    "    'scaler': scaler,\n",
    "}, model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
